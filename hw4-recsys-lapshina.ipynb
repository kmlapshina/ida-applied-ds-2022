{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExuV3ktSQYrH"
   },
   "source": [
    "<center><img src=\"https://github.com/hse-ds/iad-applied-ds/blob/master/2021/hw/hw1/img/logo_hse.png?raw=1\" width=\"1000\"></center>\n",
    "\n",
    "<h1><center>Прикладные задачи анализа данных</center></h1>\n",
    "<h2><center>Домашнее задание 4: рекомендательные системы</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef6180WvQbD-"
   },
   "source": [
    "# Введение\n",
    "\n",
    "В этом задании Вы продолжите работать с данными из семинара [Articles Sharing and Reading from CI&T Deskdrop](https://www.kaggle.com/gspmoreira/articles-sharing-reading-from-cit-deskdrop)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nSV_mxD9TciM"
   },
   "source": [
    "# Загрузка и предобработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "M5mH3ZolSlcm"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRQVuRvER0hd"
   },
   "source": [
    "Загрузим данные и проведем предобраотку данных как на семинаре."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8E837g9kQTbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "articles-sharing-reading-from-cit-deskdrop.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d gspmoreira/articles-sharing-reading-from-cit-deskdrop\n",
    "#!unzip articles-sharing-reading-from-cit-deskdrop.zip -d articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('C:/Users/User/articles-sharing-reading-from-cit-deskdrop.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('C:/Users/User')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "id": "hdM1xSchR9jt",
    "outputId": "30425539-dc45-47ef-874d-540b1c98d1f8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>eventType</th>\n",
       "      <th>contentId</th>\n",
       "      <th>authorPersonId</th>\n",
       "      <th>authorSessionId</th>\n",
       "      <th>authorUserAgent</th>\n",
       "      <th>authorRegion</th>\n",
       "      <th>authorCountry</th>\n",
       "      <th>contentType</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1459193988</td>\n",
       "      <td>CONTENT SHARED</td>\n",
       "      <td>-4110354420726924665</td>\n",
       "      <td>4340306774493623681</td>\n",
       "      <td>8940341205206233829</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HTML</td>\n",
       "      <td>http://www.nytimes.com/2016/03/28/business/dea...</td>\n",
       "      <td>Ethereum, a Virtual Currency, Enables Transact...</td>\n",
       "      <td>All of this work is still very early. The firs...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1459194146</td>\n",
       "      <td>CONTENT SHARED</td>\n",
       "      <td>-7292285110016212249</td>\n",
       "      <td>4340306774493623681</td>\n",
       "      <td>8940341205206233829</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HTML</td>\n",
       "      <td>http://cointelegraph.com/news/bitcoin-future-w...</td>\n",
       "      <td>Bitcoin Future: When GBPcoin of Branson Wins O...</td>\n",
       "      <td>The alarm clock wakes me at 8:00 with stream o...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    timestamp       eventType            contentId       authorPersonId  \\\n",
       "1  1459193988  CONTENT SHARED -4110354420726924665  4340306774493623681   \n",
       "2  1459194146  CONTENT SHARED -7292285110016212249  4340306774493623681   \n",
       "\n",
       "       authorSessionId authorUserAgent authorRegion authorCountry contentType  \\\n",
       "1  8940341205206233829             NaN          NaN           NaN        HTML   \n",
       "2  8940341205206233829             NaN          NaN           NaN        HTML   \n",
       "\n",
       "                                                 url  \\\n",
       "1  http://www.nytimes.com/2016/03/28/business/dea...   \n",
       "2  http://cointelegraph.com/news/bitcoin-future-w...   \n",
       "\n",
       "                                               title  \\\n",
       "1  Ethereum, a Virtual Currency, Enables Transact...   \n",
       "2  Bitcoin Future: When GBPcoin of Branson Wins O...   \n",
       "\n",
       "                                                text lang  \n",
       "1  All of this work is still very early. The firs...   en  \n",
       "2  The alarm clock wakes me at 8:00 with stream o...   en  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_df = pd.read_csv(\"shared_articles.csv\")\n",
    "articles_df = articles_df[articles_df[\"eventType\"] == \"CONTENT SHARED\"]\n",
    "articles_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "id": "KK9wMAkvSjbk",
    "outputId": "0824a32c-6ea9-4e22-f326-45b917b5d063"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>eventType</th>\n",
       "      <th>contentId</th>\n",
       "      <th>personId</th>\n",
       "      <th>sessionId</th>\n",
       "      <th>userAgent</th>\n",
       "      <th>userRegion</th>\n",
       "      <th>userCountry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1465413032</td>\n",
       "      <td>VIEW</td>\n",
       "      <td>-3499919498720038879</td>\n",
       "      <td>-8845298781299428018</td>\n",
       "      <td>1264196770339959068</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1465412560</td>\n",
       "      <td>VIEW</td>\n",
       "      <td>8890720798209849691</td>\n",
       "      <td>-1032019229384696495</td>\n",
       "      <td>3621737643587579081</td>\n",
       "      <td>Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2...</td>\n",
       "      <td>NY</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    timestamp eventType            contentId             personId  \\\n",
       "0  1465413032      VIEW -3499919498720038879 -8845298781299428018   \n",
       "1  1465412560      VIEW  8890720798209849691 -1032019229384696495   \n",
       "\n",
       "             sessionId                                          userAgent  \\\n",
       "0  1264196770339959068                                                NaN   \n",
       "1  3621737643587579081  Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2...   \n",
       "\n",
       "  userRegion userCountry  \n",
       "0        NaN         NaN  \n",
       "1         NY          US  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions_df = pd.read_csv(\"users_interactions.csv\")\n",
    "interactions_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5nQScdSTTzNG"
   },
   "outputs": [],
   "source": [
    "interactions_df.personId = interactions_df.personId.astype(str)\n",
    "interactions_df.contentId = interactions_df.contentId.astype(str)\n",
    "articles_df.contentId = articles_df.contentId.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "eu6R9rDQT2P4"
   },
   "outputs": [],
   "source": [
    "# зададим словарь определяющий силу взаимодействия\n",
    "event_type_strength = {\n",
    "   \"VIEW\": 1.0,\n",
    "   \"LIKE\": 2.0, \n",
    "   \"BOOKMARK\": 2.5, \n",
    "   \"FOLLOW\": 3.0,\n",
    "   \"COMMENT CREATED\": 4.0,  \n",
    "}\n",
    "\n",
    "interactions_df[\"eventStrength\"] = interactions_df.eventType.apply(lambda x: event_type_strength[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATO5PRRwUkQ0"
   },
   "source": [
    "Оставляем только тех пользователей, которые произамодействовали более чем с пятью статьями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d-0HoboYUBm5",
    "outputId": "93a6d973-136b-43c0-ab13-861742e91d59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# users: 1895\n",
      "# users with at least 5 interactions: 1140\n"
     ]
    }
   ],
   "source": [
    "users_interactions_count_df = (\n",
    "    interactions_df\n",
    "    .groupby([\"personId\", \"contentId\"])\n",
    "    .first()\n",
    "    .reset_index()\n",
    "    .groupby(\"personId\").size())\n",
    "print(\"# users:\", len(users_interactions_count_df))\n",
    "\n",
    "users_with_enough_interactions_df = \\\n",
    "    users_interactions_count_df[users_interactions_count_df >= 5].reset_index()[[\"personId\"]]\n",
    "print(\"# users with at least 5 interactions:\",len(users_with_enough_interactions_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQagI3DHUuX5"
   },
   "source": [
    "Оставляем только те взаимодействия, которые относятся к отфильтрованным пользователям."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "34rrdGdpUFgk"
   },
   "outputs": [],
   "source": [
    "interactions_from_selected_users_df = interactions_df.loc[np.in1d(interactions_df.personId,\n",
    "            users_with_enough_interactions_df)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hd3VS_BgU9HN",
    "outputId": "43c9004c-3cd9-4af7-fbc6-a26e2baf4103"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# interactions before: (72312, 9)\n",
      "# interactions after: (69868, 9)\n"
     ]
    }
   ],
   "source": [
    "print(f\"# interactions before: {interactions_df.shape}\")\n",
    "print(f\"# interactions after: {interactions_from_selected_users_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYpRiFkQVR6B"
   },
   "source": [
    "Объединяем все взаимодействия пользователя по каждой статье и сглажиываем полученный результат, взяв от него логарифм."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "mtPtAehKVEUu",
    "outputId": "187ce259-9527-45a4-9ca7-a94cac093b3d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>personId</th>\n",
       "      <th>contentId</th>\n",
       "      <th>eventStrength</th>\n",
       "      <th>last_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1007001694607905623</td>\n",
       "      <td>-5065077552540450930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1470395911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1007001694607905623</td>\n",
       "      <td>-6623581327558800021</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1487240080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1007001694607905623</td>\n",
       "      <td>-793729620925729327</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1472834892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1007001694607905623</td>\n",
       "      <td>1469580151036142903</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1487240062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1007001694607905623</td>\n",
       "      <td>7270966256391553686</td>\n",
       "      <td>1.584963</td>\n",
       "      <td>1485994324</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               personId             contentId  eventStrength  last_timestamp\n",
       "0  -1007001694607905623  -5065077552540450930       1.000000      1470395911\n",
       "1  -1007001694607905623  -6623581327558800021       1.000000      1487240080\n",
       "2  -1007001694607905623   -793729620925729327       1.000000      1472834892\n",
       "3  -1007001694607905623   1469580151036142903       1.000000      1487240062\n",
       "4  -1007001694607905623   7270966256391553686       1.584963      1485994324"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def smooth_user_preference(x):\n",
    "    return math.log(1+x, 2)\n",
    "    \n",
    "interactions_full_df = (\n",
    "    interactions_from_selected_users_df\n",
    "    .groupby([\"personId\", \"contentId\"]).eventStrength.sum()\n",
    "    .apply(smooth_user_preference)\n",
    "    .reset_index().set_index([\"personId\", \"contentId\"])\n",
    ")\n",
    "interactions_full_df[\"last_timestamp\"] = (\n",
    "    interactions_from_selected_users_df\n",
    "    .groupby([\"personId\", \"contentId\"])[\"timestamp\"].last()\n",
    ")\n",
    "        \n",
    "interactions_full_df = interactions_full_df.reset_index()\n",
    "interactions_full_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ODJYMtnNWM5w"
   },
   "source": [
    "Разобьём выборку на обучение и контроль по времени."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "3F2CfAwoVrfo",
    "outputId": "50ce92b0-0314-418f-8b85-eefc34d58727",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# interactions on Train set: 29329\n",
      "# interactions on Test set: 9777\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>personId</th>\n",
       "      <th>contentId</th>\n",
       "      <th>eventStrength</th>\n",
       "      <th>last_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1007001694607905623</td>\n",
       "      <td>-5065077552540450930</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1470395911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1007001694607905623</td>\n",
       "      <td>-793729620925729327</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1472834892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1032019229384696495</td>\n",
       "      <td>-1006791494035379303</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1469129122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1032019229384696495</td>\n",
       "      <td>-1039912738963181810</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1459376415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1032019229384696495</td>\n",
       "      <td>-1081723567492738167</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1464054093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               personId             contentId  eventStrength  last_timestamp\n",
       "0  -1007001694607905623  -5065077552540450930            1.0      1470395911\n",
       "2  -1007001694607905623   -793729620925729327            1.0      1472834892\n",
       "6  -1032019229384696495  -1006791494035379303            1.0      1469129122\n",
       "7  -1032019229384696495  -1039912738963181810            1.0      1459376415\n",
       "8  -1032019229384696495  -1081723567492738167            2.0      1464054093"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "split_ts = 1475519530\n",
    "interactions_train_df = interactions_full_df.loc[interactions_full_df.last_timestamp < split_ts].copy()\n",
    "interactions_test_df = interactions_full_df.loc[interactions_full_df.last_timestamp >= split_ts].copy()\n",
    "\n",
    "print(f\"# interactions on Train set: {len(interactions_train_df)}\")\n",
    "print(f\"# interactions on Test set: {len(interactions_test_df)}\")\n",
    "\n",
    "interactions_train_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5G3FTYOXLVg"
   },
   "source": [
    "Для удобства подсчёта качества запишем данные в формате, где строка соответствует пользователю, а столбцы будут истинными метками и предсказаниями в виде списков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "id": "RT-_toqfXOa2",
    "outputId": "0365644b-bd79-4106-fb04-5f5b1d7c4b61"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true_train</th>\n",
       "      <th>true_test</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>personId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1007001694607905623</th>\n",
       "      <td>[-5065077552540450930, -793729620925729327]</td>\n",
       "      <td>[-6623581327558800021, 1469580151036142903, 72...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1032019229384696495</th>\n",
       "      <td>[-1006791494035379303, -1039912738963181810, -...</td>\n",
       "      <td>[-1415040208471067980, -2555801390963402198, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-108842214936804958</th>\n",
       "      <td>[-1196068832249300490, -133139342397538859, -1...</td>\n",
       "      <td>[-2780168264183400543, -3060116862184714437, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1130272294246983140</th>\n",
       "      <td>[-1150591229250318592, -1196068832249300490, -...</td>\n",
       "      <td>[-1606980109000976010, -1663441888197894674, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1160159014793528221</th>\n",
       "      <td>[-133139342397538859, -387651900461462767, 377...</td>\n",
       "      <td>[-3462051751080362224]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             true_train  \\\n",
       "personId                                                                  \n",
       "-1007001694607905623        [-5065077552540450930, -793729620925729327]   \n",
       "-1032019229384696495  [-1006791494035379303, -1039912738963181810, -...   \n",
       "-108842214936804958   [-1196068832249300490, -133139342397538859, -1...   \n",
       "-1130272294246983140  [-1150591229250318592, -1196068832249300490, -...   \n",
       "-1160159014793528221  [-133139342397538859, -387651900461462767, 377...   \n",
       "\n",
       "                                                              true_test  \n",
       "personId                                                                 \n",
       "-1007001694607905623  [-6623581327558800021, 1469580151036142903, 72...  \n",
       "-1032019229384696495  [-1415040208471067980, -2555801390963402198, -...  \n",
       "-108842214936804958   [-2780168264183400543, -3060116862184714437, -...  \n",
       "-1130272294246983140  [-1606980109000976010, -1663441888197894674, -...  \n",
       "-1160159014793528221                             [-3462051751080362224]  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions = (\n",
    "    interactions_train_df\n",
    "    .groupby(\"personId\")[\"contentId\"].agg(lambda x: list(x))\n",
    "    .reset_index()\n",
    "    .rename(columns={\"contentId\": \"true_train\"})\n",
    "    .set_index(\"personId\")\n",
    ")\n",
    "\n",
    "interactions[\"true_test\"] = (\n",
    "    interactions_test_df\n",
    "    .groupby(\"personId\")[\"contentId\"].agg(lambda x: list(x))\n",
    ")\n",
    "\n",
    "# заполнение пропусков пустыми списками\n",
    "interactions.loc[pd.isnull(interactions.true_test), \"true_test\"] = [\n",
    "    \"\" for x in range(len(interactions.loc[pd.isnull(interactions.true_test), \"true_test\"]))]\n",
    "\n",
    "interactions.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-UWDyWKsamSa"
   },
   "source": [
    "# Библиотека LightFM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L-iXjvdZa25Z"
   },
   "source": [
    "Для рекомендации Вы будете пользоваться библиотекой [LightFM](https://making.lyst.com/lightfm/docs/home.html), в которой реализованы популярные алгоритмы. Для оценивания качества рекомендации, как и на семинаре, будем пользоваться метрикой *precision@10*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\ANACONDA\\lib\\site-packages\\lightfm\\_lightfm_fast.py:9: UserWarning: LightFM was compiled without OpenMP support. Only a single thread will be used.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#conda install -c conda-forge lightfm\n",
    "#conda update -n base -c defaults conda\n",
    "\n",
    "from lightfm import LightFM\n",
    "from lightfm.evaluation import precision_at_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0CjyGqulcZTf"
   },
   "source": [
    "## Задание 1 (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Sof6V5Dd4h9"
   },
   "source": [
    "Модели в LightFM работают с разреженными матрицами. Создайте разреженные матрицы `data_train` и `data_test` (размером количество пользователей на количество статей), такие что на пересечении строки пользователя и столбца статьи стоит сила их взаимодействия, если взаимодействие было, и стоит ноль, если взаимодействия не было."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://medium.com/swlh/an-in-depth-introduction-to-sparse-matrix-a5972d7e8c86\n",
    "\n",
    "all_users = np.unique(np.hstack((interactions_train_df.personId.unique(), interactions_test_df.personId.unique())))\n",
    "all_articles =  np.unique(np.hstack((interactions_train_df.contentId.unique(), interactions_test_df.contentId.unique())))\n",
    "\n",
    "# Пустые матрицы одинакового размера\n",
    "matrix_train = pd.DataFrame(0, columns=all_articles, index=all_users)\n",
    "matrix_test = matrix_train.copy()\n",
    "\n",
    "# Заполняю\n",
    "for i in range(interactions_train_df.shape[0]):\n",
    "    strength = np.array(interactions_train_df.eventStrength)[i]\n",
    "    if strength != np.nan:\n",
    "        matrix_train.loc[np.array(interactions_train_df.personId)[i], np.array(interactions_train_df.contentId)[i]] = strength\n",
    "    else:\n",
    "        matrix_train.loc[np.array(interactions_train_df.personId)[i], np.array(interactions_train_df.contentId)[i]] = 0\n",
    "        \n",
    "for i in range(interactions_test_df.shape[0]):\n",
    "    strength = np.array(interactions_test_df.eventStrength)[i]\n",
    "    if strength != np.nan:\n",
    "        matrix_test.loc[np.array(interactions_test_df.personId)[i], np.array(interactions_test_df.contentId)[i]] = strength\n",
    "    else:\n",
    "        matrix_test.loc[np.array(interactions_test_df.personId)[i], np.array(interactions_test_df.contentId)[i]] = 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = csr_matrix(np.array(matrix_train))\n",
    "data_test = csr_matrix(np.array(matrix_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert data_train.shape == data_test.shape\n",
    "assert data_train.shape[0] == all_users.shape[0] and data_train.shape[1] == all_articles.shape[0]\n",
    "assert data_test.shape[0] == all_users.shape[0] and data_test.shape[1] == all_articles.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AiNGVzTveRXE"
   },
   "source": [
    "## Задание 2 (1 балл)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPfVK3STeryM"
   },
   "source": [
    "Обучите модель LightFM с `loss=\"warp\"` и посчитайте *precision@10* на тесте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision на трейне: 0.21708633 \n",
      "Precision на тесте: 0.0066191447\n",
      "Wall time: 3.31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lfm = LightFM(loss='warp')\n",
    "lfm.fit(data_train, epochs=20)\n",
    "\n",
    "pr_train = precision_at_k(model = lfm, \n",
    "                          test_interactions = data_train, \n",
    "                          k=10).mean()\n",
    "pr_test = precision_at_k(model = lfm, \n",
    "                         test_interactions = data_test, \n",
    "                         train_interactions=data_train, \n",
    "                         k=10).mean()\n",
    "\n",
    "print('Precision на трейне:', pr_train, '\\nPrecision на тесте:', pr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Буду сохранять все пресижены\n",
    "saved_scores = pd.DataFrame(columns = ['Model', 'Precision on train', 'Precision on test'])\n",
    "saved_scores = saved_scores.append({'Model': 'Baseline: LightFM with loss = warp',\n",
    "                                    'Precision on train': pr_train, 'Precision on test': pr_test}, ignore_index = True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qZDsG1iAfdrl"
   },
   "source": [
    "## Задание 3 (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F93chvtbgA9N"
   },
   "source": [
    "При вызове метода `fit` LightFM позволяет передавать в `item_features` признаковое описание объектов. Воспользуемся этим. Будем получать признаковое описание из текста статьи в виде [TF-IDF](https://ru.wikipedia.org/wiki/TF-IDF) (можно воспользоваться `TfidfVectorizer` из scikit-learn). Создайте матрицу `feat` размером количесвто статей на размер признакового описание и обучите LightFM с `loss=\"warp\"` и посчитайте precision@10 на тесте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision на трейне: 0.23570144 \n",
      "Precision на тесте: 0.006720978\n",
      "Wall time: 6min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "feat = vectorizer.fit_transform(articles_df.text)\n",
    "\n",
    "lfm = LightFM(loss='warp')\n",
    "lfm.fit(data_train, epochs=20, item_features = feat)\n",
    "\n",
    "pr_train = precision_at_k(model = lfm, \n",
    "                          test_interactions = data_train,\n",
    "                          item_features = feat,\n",
    "                          k=10).mean()\n",
    "pr_test = precision_at_k(model = lfm, \n",
    "                         test_interactions = data_test,\n",
    "                         item_features = feat,\n",
    "                         train_interactions=data_train, \n",
    "                         k=10).mean()\n",
    "\n",
    "print('Precision на трейне:', pr_train, '\\nPrecision на тесте:', pr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_scores = saved_scores.append({'Model': 'Add TF-IDF',\n",
    "                                    'Precision on train': pr_train, 'Precision on test': pr_test}, ignore_index = True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Lwuex6PpsFw"
   },
   "source": [
    "## Задание 4 (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5aZcHjSzp2cZ"
   },
   "source": [
    "В задании 3 мы использовали сырой текст статей. В этом задании необходимо сначала сделать предобработку текста (привести к нижнему регистру, убрать стоп слова, привести слова к номральной форме и т.д.), после чего обучите модель и оценить качество на тестовых данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#https://github.com/stopwords-iso/stopwords-iso\n",
    "#https://www.geeksforgeeks.org/python-lemmatization-approaches-with-examples/\n",
    "#https://stackoverflow.com/questions/24647400/what-is-the-best-stemming-method-in-python\n",
    "\n",
    "#pip install stopwordsiso\n",
    "import stopwordsiso as stpwrds\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en    2211\n",
       "pt     829\n",
       "la       3\n",
       "ja       2\n",
       "es       2\n",
       "Name: lang, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langs = list(articles_df.lang.unique())\n",
    "articles_df.lang.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_all = stpwrds.stopwords(langs)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Токенайзер который делает bare minimum\n",
    "def my_tokenizer(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens \\\n",
    "               if token not in stopwords_all and token.isalpha() == True]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision на трейне: 0.24145684 \n",
      "Precision на тесте: 0.0065173116\n",
      "Wall time: 3min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer = my_tokenizer)\n",
    "feat = vectorizer.fit_transform(articles_df.text)\n",
    "lfm = LightFM(loss='warp')\n",
    "lfm.fit(data_train, epochs=20, item_features = feat)\n",
    "\n",
    "pr_train = precision_at_k(model = lfm, \n",
    "                          test_interactions = data_train,\n",
    "                          item_features = feat,\n",
    "                          k=10).mean()\n",
    "pr_test = precision_at_k(model = lfm, \n",
    "                         test_interactions = data_test,\n",
    "                         item_features = feat,\n",
    "                         train_interactions=data_train, \n",
    "                         k=10).mean()\n",
    "\n",
    "print('Precision на трейне:', pr_train, '\\nPrecision на тесте:', pr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_scores = saved_scores.append({'Model': 'TF-IDF + custom tokenizer (lemma)',\n",
    "                                    'Precision on train': pr_train, 'Precision on test': pr_test}, ignore_index = True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VgayAPRpqn7L"
   },
   "source": [
    "**Улучшилось ли качество предсказания?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Нет, не улучшилось (и вообще улучшается / ухудшается рандомно при разных запусках тетрадки), у меня есть подозрение, что WordNetLemmatizer может плохо справляться с текстами не на английском языке, хотя по документации предполагается, что он без проблем берет лемму от любого слова*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['artigos',\n",
       " 'palestra',\n",
       " 'artigos',\n",
       " 'perspectivas',\n",
       " 'agronegócio',\n",
       " 'demandam',\n",
       " 'tecnologias',\n",
       " 'produtividade',\n",
       " 'sustentável',\n",
       " 'agricultura',\n",
       " 'precisão',\n",
       " 'alimentar',\n",
       " 'bilhões',\n",
       " 'artigos',\n",
       " 'adoção',\n",
       " 'agricultura',\n",
       " 'precisão',\n",
       " 'brasil',\n",
       " 'alberto',\n",
       " 'campos',\n",
       " 'bernardi',\n",
       " 'embrapa',\n",
       " 'pecuária',\n",
       " 'sudeste',\n",
       " 'ricardo',\n",
       " 'inamasu',\n",
       " 'embrapa',\n",
       " 'instrumentação',\n",
       " 'artigos',\n",
       " 'agricultura',\n",
       " 'precisão',\n",
       " 'ferramenta',\n",
       " 'alcance',\n",
       " 'alberto',\n",
       " 'bernardi',\n",
       " 'pesquisador',\n",
       " 'embrapa',\n",
       " 'pecuária',\n",
       " 'sudeste',\n",
       " 'formado',\n",
       " 'agronomia',\n",
       " 'mestrado',\n",
       " 'nutrição',\n",
       " 'plantas',\n",
       " 'doutorado',\n",
       " 'nutrição',\n",
       " 'plantas',\n",
       " 'fertilidade',\n",
       " 'adubação',\n",
       " 'integração',\n",
       " 'agricultura',\n",
       " 'precisão',\n",
       " 'ricardo',\n",
       " 'inamasu',\n",
       " 'pesquisador',\n",
       " 'embrapa',\n",
       " 'instrumentação',\n",
       " 'graduação',\n",
       " 'mestrado',\n",
       " 'doutorado',\n",
       " 'engenharia',\n",
       " 'mecânica',\n",
       " 'escola',\n",
       " 'engenharia',\n",
       " 'carlos',\n",
       " 'usp',\n",
       " 'biological',\n",
       " 'system',\n",
       " 'engineering',\n",
       " 'university',\n",
       " 'nebraska',\n",
       " 'lincoln',\n",
       " 'experiência',\n",
       " 'engenharia',\n",
       " 'mecânica',\n",
       " 'mecatrônica',\n",
       " 'ênfase',\n",
       " 'instrumentação',\n",
       " 'automação',\n",
       " 'agropecuária',\n",
       " 'tecnologia',\n",
       " 'inovação',\n",
       " 'agropecuária',\n",
       " 'brasileira',\n",
       " 'renato',\n",
       " 'roscoe',\n",
       " 'engenheiro',\n",
       " 'agrônomo',\n",
       " 'ufv',\n",
       " 'mestre',\n",
       " 'ciência',\n",
       " 'ufla',\n",
       " 'doutor',\n",
       " 'environmental',\n",
       " 'science',\n",
       " 'wageningen',\n",
       " 'university',\n",
       " 'centre',\n",
       " 'atua',\n",
       " 'diretor',\n",
       " 'executivo',\n",
       " 'pesquisador',\n",
       " 'setor',\n",
       " 'fertilidade',\n",
       " 'fundação',\n",
       " 'josé',\n",
       " 'luis',\n",
       " 'silva',\n",
       " 'nunes',\n",
       " 'fitotecnia',\n",
       " 'universidade',\n",
       " 'federal',\n",
       " 'rio',\n",
       " 'sul',\n",
       " 'joão',\n",
       " 'guilherme',\n",
       " 'sabino',\n",
       " 'ometto',\n",
       " 'fiesp',\n",
       " 'artigo',\n",
       " 'publicado',\n",
       " 'jornal',\n",
       " 'paulo',\n",
       " 'antônio',\n",
       " 'luis',\n",
       " 'jackson',\n",
       " 'ernani',\n",
       " 'kassia',\n",
       " 'luiza',\n",
       " 'teixeira',\n",
       " 'maurício',\n",
       " 'roberto',\n",
       " 'mateus',\n",
       " 'tonini',\n",
       " 'telmo',\n",
       " 'jorge',\n",
       " 'carneiro',\n",
       " 'fabio',\n",
       " 'evandro',\n",
       " 'grub',\n",
       " 'vitor',\n",
       " 'cauduro',\n",
       " 'girardello',\n",
       " 'telmo',\n",
       " 'jorge',\n",
       " 'carneiro',\n",
       " 'amado',\n",
       " 'rodrigo',\n",
       " 'silveira',\n",
       " 'nicoloso',\n",
       " 'tiago',\n",
       " 'andrade',\n",
       " 'neve',\n",
       " 'hörbe',\n",
       " 'ademir',\n",
       " 'oliveira',\n",
       " 'ferreira',\n",
       " 'fabiano',\n",
       " 'mauricio',\n",
       " 'tabaldi',\n",
       " 'mastrângello',\n",
       " 'enívar',\n",
       " 'lanzanova',\n",
       " 'joão',\n",
       " 'augusto',\n",
       " 'telles',\n",
       " 'presidente',\n",
       " 'comissão',\n",
       " 'irrigantes',\n",
       " 'farsul',\n",
       " 'coordenador',\n",
       " 'clube',\n",
       " 'irrigação',\n",
       " 'chefe',\n",
       " 'divisão',\n",
       " 'técnica',\n",
       " 'telmo',\n",
       " 'amado',\n",
       " 'antônio',\n",
       " 'santi',\n",
       " 'professores',\n",
       " 'mestrado',\n",
       " 'profissional',\n",
       " 'agricultura',\n",
       " 'precisão',\n",
       " 'ufsm',\n",
       " 'tiago',\n",
       " 'horbe',\n",
       " 'tiagohorbe',\n",
       " 'ademir',\n",
       " 'ferreira',\n",
       " 'doutorandos',\n",
       " 'ciência',\n",
       " 'josé',\n",
       " 'paulo',\n",
       " 'molin',\n",
       " 'palestrantes',\n",
       " 'seminários',\n",
       " 'agricultura',\n",
       " 'precisão',\n",
       " 'senar',\n",
       " 'professor',\n",
       " 'associado',\n",
       " 'departamento',\n",
       " 'engenharia',\n",
       " 'biossistemas',\n",
       " 'mecânica',\n",
       " 'máquinas',\n",
       " 'agrícolas',\n",
       " 'ricardo',\n",
       " 'inamasu',\n",
       " 'palestrantes',\n",
       " 'seminários',\n",
       " 'agricultura',\n",
       " 'precisão',\n",
       " 'senar',\n",
       " 'pesquisador',\n",
       " 'embrapa',\n",
       " 'instrumentação',\n",
       " 'professor',\n",
       " 'colaborador',\n",
       " 'escola',\n",
       " 'engenharia',\n",
       " 'carlos',\n",
       " 'vinculada',\n",
       " 'universidade',\n",
       " 'paulo',\n",
       " 'usp',\n",
       " 'palestra',\n",
       " 'agricultura',\n",
       " 'precisão',\n",
       " 'situação',\n",
       " 'tendências',\n",
       " 'jósé',\n",
       " 'molin',\n",
       " 'agricultura',\n",
       " 'precisão',\n",
       " 'base',\n",
       " 'conceitual',\n",
       " 'ricardo',\n",
       " 'inamasu',\n",
       " 'embrapa',\n",
       " 'manejo',\n",
       " 'lavoura',\n",
       " 'altas',\n",
       " 'produtividades',\n",
       " 'base',\n",
       " 'agricultura',\n",
       " 'precisão',\n",
       " 'telmo',\n",
       " 'amado']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Проверяю токенайзер на статье на португальском языке (таких у нас много -- больше 800)\n",
    "pt_texts = np.array(articles_df[articles_df.lang == 'pt'].text)\n",
    "my_tokenizer(pt_texts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Выглядит не очень, у слов как будто остались окончания множественного числа или окончания, обозначающие гендер*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Перепишу токенайзер со стеммингом вместо лемматизации\n",
    "def my_tokenizer_stem(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    tokens = [stemmer.stem(token) for token in tokens \\\n",
    "               if token not in stopwords_all and token.isalpha() == True]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision на трейне: 0.2365108 \n",
      "Precision на тесте: 0.0066191447\n",
      "Wall time: 4min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer = my_tokenizer_stem)\n",
    "feat = vectorizer.fit_transform(articles_df.text)\n",
    "lfm = LightFM(loss='warp')\n",
    "lfm.fit(data_train, epochs=20, item_features = feat)\n",
    "\n",
    "pr_train = precision_at_k(model = lfm, \n",
    "                          test_interactions = data_train,\n",
    "                          item_features = feat,\n",
    "                          k=10).mean()\n",
    "pr_test = precision_at_k(model = lfm, \n",
    "                         test_interactions = data_test,\n",
    "                         item_features = feat,\n",
    "                         train_interactions=data_train, \n",
    "                         k=10).mean()\n",
    "\n",
    "print('Precision на трейне:', pr_train, '\\nPrecision на тесте:', pr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_scores = saved_scores.append({'Model': 'TF-IDF + custom tokenizer (stem)',\n",
    "                                    'Precision on train': pr_train, 'Precision on test': pr_test}, ignore_index = True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Почти ничего не изменилось, ну и ладно*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKRzLLodq3gq"
   },
   "source": [
    "## Задание 5 (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "brjHl49Aq8su"
   },
   "source": [
    "Подберите гиперпараметры модели LightFM (`n_components` и др.) для улучшения качества модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/49896816/how-do-i-optimize-the-hyperparameters-of-lightfm\n",
    "\n",
    "import itertools\n",
    "\n",
    "def sample_hyperparameters():\n",
    "    \n",
    "    while True:\n",
    "        yield {\n",
    "            \"no_components\": np.random.randint(16, 64),\n",
    "            \"learning_schedule\": np.random.choice([\"adagrad\", \"adadelta\"]),\n",
    "            \"learning_rate\": np.random.exponential(0.05),\n",
    "            \"item_alpha\": np.random.exponential(1e-8),\n",
    "            \"user_alpha\": np.random.exponential(1e-8),\n",
    "            \"num_epochs\": np.random.randint(5, 50),\n",
    "        }\n",
    "\n",
    "def random_search(train, test, num_samples=10):\n",
    "\n",
    "    for hyperparams in itertools.islice(sample_hyperparameters(), num_samples):\n",
    "        num_epochs = hyperparams.pop(\"num_epochs\")\n",
    "\n",
    "        model = LightFM(**hyperparams, loss='warp')\n",
    "        model.fit(train, epochs=num_epochs, item_features = feat)\n",
    "\n",
    "        #score = auc_score(model, test, train_interactions=train, num_threads=num_threads).mean()\n",
    "        score = precision_at_k(model = model,\n",
    "                               test_interactions = test,\n",
    "                               item_features = feat,\n",
    "                               train_interactions=train, \n",
    "                               k=10).mean()\n",
    "\n",
    "        hyperparams[\"num_epochs\"] = num_epochs\n",
    "\n",
    "        yield (score, hyperparams, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score 0.007331975270062685 at {'no_components': 29, 'learning_schedule': 'adadelta', 'learning_rate': 0.04058360639561338, 'item_alpha': 5.616316422287524e-08, 'user_alpha': 3.3506621712832566e-08, 'num_epochs': 29}\n",
      "Wall time: 2h 17min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "(score, hyperparams, model) = max(random_search(data_train, data_test), key=lambda x: x[0])\n",
    "print(\"Best score {} at {}\".format(score, hyperparams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_scores = saved_scores.append({'Model': 'Random search on hyperparam -- best model',\n",
    "                                    'Precision on train': '', 'Precision on test': score}, ignore_index = True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраняю гиперпараметры для следующего задания\n",
    "best_hp = hyperparams.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xJA7v07NrYSF"
   },
   "source": [
    "## Бонусное задание (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "veLFUoVRrisk"
   },
   "source": [
    "Выше мы использовали достаточно простое представление текста статьи в виде TF-IDF. В этом задании Вам нужно представить текст статьи (можно вместе с заголовком) в виде эмбеддинга полученного с помощью рекуррентной сети или трансформера (можно использовать любую предобученную модель, которая Вам нравится). Обучите модель с ипользованием этих эмеддингов и сравните результаты с предыдущими."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install sentence_transformers\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://medium.com/swlh/transformer-based-sentence-embeddings-cd0935b3b1e0\n",
    "#https://sbert.net/examples/applications/computing-embeddings/README.html#sentence-embeddings-with-transformers\n",
    "\n",
    "transformer = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = transformer.encode(np.array(articles_df.text))\n",
    "embeddings = csr_matrix(embeddings)\n",
    "\n",
    "assert embeddings.shape[0] == feat.shape[0]\n",
    "assert type(embeddings) == type(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision на трейне: 0.18408275 \n",
      "Precision на тесте: 0.004378819\n",
      "Wall time: 48min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "n_epochs = best_hp.pop('num_epochs')\n",
    "lfm = LightFM(**best_hp, loss='warp')\n",
    "lfm.fit(data_train, epochs = n_epochs, item_features = embeddings)\n",
    "\n",
    "pr_train = precision_at_k(model = lfm, \n",
    "                          test_interactions = data_train,\n",
    "                          item_features = embeddings,\n",
    "                          k=10).mean()\n",
    "pr_test = precision_at_k(model = lfm, \n",
    "                         test_interactions = data_test,\n",
    "                         item_features = embeddings,\n",
    "                         train_interactions=data_train, \n",
    "                         k=10).mean()\n",
    "\n",
    "print('Precision на трейне:', pr_train, '\\nPrecision на тесте:', pr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_scores = saved_scores.append({'Model': 'Best model (prev), add sentence transformer',\n",
    "                                    'Precision on train': pr_train, 'Precision on test': pr_test}, ignore_index = True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Precision on train</th>\n",
       "      <th>Precision on test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baseline: LightFM with loss = warp</td>\n",
       "      <td>0.217086</td>\n",
       "      <td>0.006619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Add TF-IDF</td>\n",
       "      <td>0.235701</td>\n",
       "      <td>0.006721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TF-IDF + custom tokenizer (lemma)</td>\n",
       "      <td>0.241457</td>\n",
       "      <td>0.006517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TF-IDF + custom tokenizer (stem)</td>\n",
       "      <td>0.236511</td>\n",
       "      <td>0.006619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random search on hyperparam -- best model</td>\n",
       "      <td></td>\n",
       "      <td>0.007332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Best model (prev), add sentence transformer</td>\n",
       "      <td>0.184083</td>\n",
       "      <td>0.004379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Model Precision on train  \\\n",
       "0           Baseline: LightFM with loss = warp           0.217086   \n",
       "1                                   Add TF-IDF           0.235701   \n",
       "2            TF-IDF + custom tokenizer (lemma)           0.241457   \n",
       "3             TF-IDF + custom tokenizer (stem)           0.236511   \n",
       "4    Random search on hyperparam -- best model                      \n",
       "5  Best model (prev), add sentence transformer           0.184083   \n",
       "\n",
       "   Precision on test  \n",
       "0           0.006619  \n",
       "1           0.006721  \n",
       "2           0.006517  \n",
       "3           0.006619  \n",
       "4           0.007332  \n",
       "5           0.004379  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Сравнение результатов\n",
    "saved_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "HW3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
